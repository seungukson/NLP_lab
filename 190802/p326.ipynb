{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상한 나라의 엘리스 소설을 읽어온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice_in_wonderland.txt', 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "\n",
    "content2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in content]).split())\n",
    " \n",
    "tokens = nltk.word_tokenize(content2)\n",
    "tokens = [word.lower() for word in tokens if len(word)>=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "quads = list(nltk.ngrams(tokens, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alice', 'adventures', 'in'),\n",
       " ('adventures', 'in', 'wonderland'),\n",
       " ('in', 'wonderland', 'alice')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quads[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newl_app = []\n",
    "for ln in quads:\n",
    "    newl = \" \".join(ln)\n",
    "    newl_app.append(newl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alice adventures in', 'adventures in wonderland', 'in wonderland alice']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newl_app[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_trigm = []\n",
    "y_trigm = []\n",
    "\n",
    "for l in newl_app:\n",
    "    x_str = \" \".join(l.split()[0:N-1])  # trigram 중 앞 부분 2 단어. \n",
    "    y_str = l.split()[N-1]              # trigram 중 마지막 1 단어.  \n",
    "    x_trigm.append(x_str)\n",
    "    y_trigm.append(y_str)\n",
    "\n",
    "x_trigm_check = vectorizer.fit_transform(x_trigm).todense()\n",
    "y_trigm_check = vectorizer.fit_transform(y_trigm).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries from word to integer and integer to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape (17947, 2559) Y Train shape (17947, 2559)\n",
      "X Test shape (7692, 2559) Y Test shape (7692, 2559)\n"
     ]
    }
   ],
   "source": [
    "dictnry = vectorizer.vocabulary_\n",
    "rev_dictnry = {v:k for k,v in dictnry.items()}\n",
    "\n",
    "X = np.array(x_trigm_check)\n",
    "Y = np.array(y_trigm_check)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest,xtrain_tg,xtest_tg = train_test_split(X, Y,x_trigm, test_size=0.3,random_state=42)\n",
    "\n",
    "print(\"X Train shape\",Xtrain.shape, \"Y Train shape\" , Ytrain.shape)\n",
    "print(\"X Test shape\",Xtest.shape, \"Y Test shape\" , Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\seong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\seong\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2559)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              2560000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 800)               800800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              801000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "fourth (Dense)               (None, 2559)              2561559   \n",
      "=================================================================\n",
      "Total params: 6,723,359\n",
      "Trainable params: 6,723,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "input_layer = Input(shape = (Xtrain.shape[1],))\n",
    "first_layer = Dense(1000, activation='relu')(input_layer)\n",
    "first_dropout = Dropout(rate = 0.5)(first_layer)\n",
    "\n",
    "second_layer = Dense(800, activation='relu')(first_dropout)\n",
    "\n",
    "third_layer = Dense(1000, activation='relu')(second_layer)\n",
    "third_dropout = Dropout(rate = 0.5)(third_layer)\n",
    "\n",
    "fourth_layer = Dense(Ytrain.shape[1], activation='softmax',name = \"fourth\")(third_dropout)\n",
    "\n",
    "\n",
    "history = Model(input_layer, fourth_layer)\n",
    "history.compile(optimizer = \"adam\",loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print (history.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14357 samples, validate on 3590 samples\n",
      "Epoch 1/10\n",
      "14357/14357 [==============================] - 24s 2ms/step - loss: 2.5097 - acc: 0.4333 - val_loss: 7.4453 - val_acc: 0.1031\n",
      "Epoch 2/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.4176 - acc: 0.4387 - val_loss: 7.5718 - val_acc: 0.1019\n",
      "Epoch 3/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.3286 - acc: 0.4596 - val_loss: 7.5107 - val_acc: 0.1064\n",
      "Epoch 4/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.2514 - acc: 0.4679 - val_loss: 7.7205 - val_acc: 0.1050\n",
      "Epoch 5/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.1901 - acc: 0.4750 - val_loss: 7.7672 - val_acc: 0.1058\n",
      "Epoch 6/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.1134 - acc: 0.4888 - val_loss: 7.7929 - val_acc: 0.1070\n",
      "Epoch 7/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 2.0519 - acc: 0.4971 - val_loss: 7.8076 - val_acc: 0.1028\n",
      "Epoch 8/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 1.9895 - acc: 0.5042 - val_loss: 7.9409 - val_acc: 0.1078\n",
      "Epoch 9/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 1.9395 - acc: 0.5154 - val_loss: 8.0591 - val_acc: 0.1050\n",
      "Epoch 10/10\n",
      "14357/14357 [==============================] - 23s 2ms/step - loss: 1.8963 - acc: 0.5269 - val_loss: 8.0487 - val_acc: 0.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9e60d18d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = history.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample check on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior bigram words | Actual | Predicted \n",
      "\n",
      "0 the evening | beautiful | beautiful  turtle  rabbit  hall  little  \n",
      "1 slipped in | like | and  in  the  with  half  \n",
      "2 alice swallowing | down | good  her  very  little  not  \n",
      "3 an encouraging | tone | of  to  but  again  voice  \n",
      "4 waistcoat pocket | or | went  music  and  sat  repeated  \n",
      "5 she went | on | on  down  back  hunting  up  \n",
      "6 that she | knew | was  had  would  got  said  \n",
      "7 down on | her | one  the  his  their  her  \n",
      "8 dormouse went | on | on  down  after  without  back  \n",
      "9 soup soup | of | you  said  and  to  in  \n"
     ]
    }
   ],
   "source": [
    "print (\"Prior bigram words\",\"| Actual\",\"| Predicted\",\"\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    print (i,xtest_tg[i], \"|\", rev_dictnry[np.argmax(Ytest[i])], \"| \", end='')\n",
    "    idx = np.flipud(Y_pred[i].argsort())[:5]\n",
    "    for k in idx:\n",
    "        print(rev_dictnry[k], ' ', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
